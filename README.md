# ğŸŒŒ NASA Exoplanet Detection AI

Advanced machine learning system to identify exoplanet candidates from NASA mission data (Kepler, K2, TESS). Ships with an ensemble pipeline and a Streamlit web app for manual and batch predictions, model comparison, explainability (SHAP), and external validation.

## âœ¨ Features
- Ensemble models: LightGBM, RandomForest, ExtraTrees, AdaBoost, XGBoost, Stacking
- Perâ€‘prediction model selection + â€œCompare all modelsâ€
- CSV batch predictions with optional multiâ€‘model comparison
- External Validation page (TOI & K2)
- Threshold slider + optional probability calibration
- SHAP explanation for a single sample (classâ€‘1 selection and robust plotting)
- Retrain/Tuning panel (quick RandomForest/LightGBM params)
- Export HTML performance report

## ğŸ“ Project Structure
```
.
â”œâ”€ data_preprocessing.py   # Loading, cleaning, feature engineering, scaling/imputing, external feature mapping
â”œâ”€ models.py               # Create/train/evaluate/save/load models; predict
â”œâ”€ train_models.py         # Endâ€‘toâ€‘end training + reports
â”œâ”€ web_app.py              # Streamlit app (UI)
â”œâ”€ normalize_external_csvs.py # Utility to normalize TOI/K2 CSV copies
â”œâ”€ generate_test_data.py   # Mock CSV generators
â”œâ”€ reports/                # Generated plots and text report
â”œâ”€ *.csv                   # KOI / TOI / K2 datasets (NASA)
â””â”€ exoplanet_models_*.joblib
```

## ğŸš€ Quick Start
1) Create the environment and install dependencies
```bash
python -m venv nasa_exoplanet_env
nasa_exoplanet_env\Scripts\activate
pip install -r requirements.txt
```
2) Place datasets in the project root (exact names):
- `Kepler Objects of Interest (KOI).csv`
- `TESS Objects of Interest (TOI).csv`
- `K2 Planets and Candidates.csv`

3) (Optional) Train models if joblib files arenâ€™t present
```bash
python train_models.py
```

4) Run the app
```bash
streamlit run web_app.py
```
Open `http://localhost:8501`.

## ğŸ–¥ï¸ App Pages
- Home: overview & quick stats
- Make Predictions:
  - Manual Entry: input parameters, choose model, set threshold, optional calibration, compare all models
  - Upload CSV: batch predictions; choose model or compare all; download results
- Model Performance: metrics table/charts; export HTML report
- Feature Analysis: feature importances; SHAP (single sample)
- External Validation: metrics on independent TOI & K2 datasets
- About: background & implementation

## ğŸ§ª External Validation (TOI & K2)
The app validates the best model on TESS TOI and K2 datasets and reports Accuracy/Precision/Recall/F1/AUC.

Data requirements (strict, caseâ€‘sensitive):
- TOI file must contain column `tfopwg_disp` with values in {`CP`, `PC`, `FP`}.
- K2 file must contain column `disposition` with values in {`CONFIRMED`, `CANDIDATE`, `FALSE POSITIVE`}.

Feature mapping to KOI schema happens automatically for TOI/K2 (e.g., `pl_orbperâ†’koi_period`, `pl_trandurhâ†’koi_duration`, `st_teffâ†’koi_steff`, etc.). If `pl_trandurh` is missing, `pl_trandur` is used; if both are missing, a safe placeholder is injected so transforms succeed.

Copies and fallback:
- If present, the app prefers the copy files in the project root:
  - `TESS Objects of Interest (TOI) copy.csv`
  - `K2 Planets and Candidates copy.csv`
  Otherwise it uses the original filenames.

Normalization utility (optional but recommended):
```bash
python normalize_external_csvs.py
```
This standardizes label values, trims whitespace, and tries to fill missing `ra/dec` from common alternatives. It operates inâ€‘place on the copy files.

Troubleshooting â€œNo external validation data availableâ€:
- Open â€œMake Predictionsâ€ once to initialize the processor (fits scalers and establishes the training feature schema), then revisit External Validation.
- Ensure required files and label values as above; run the normalizer if needed.

## âš™ï¸ Threshold & Calibration
- Threshold: tradeâ€‘off recall vs precision (more candidates vs fewer false alarms)
- Calibration (isotonic): makes probabilities better calibrated for ranking/thresholding

## ğŸ§© SHAP (single sample)
The app selects class 1 (positive) explanation and uses robust waterfall rendering. If a metaâ€‘ensemble has limited SHAP support, a tree model is preferred where applicable.

## ğŸ” Retrain/Tuning
- Retrain from the sidebar.
- Quick tuning updates parameters on already loaded models (no recreation) and recomputes scores.

## ğŸ§ª Training & Reports
`python train_models.py` trains all models on KOI, evaluates (holdâ€‘out + CV), validates externally, and saves:
- Models: `exoplanet_models_*.joblib`
- Reports: `reports/model_performance_report.png`, `reports/performance_report.txt`

## ğŸ§° Test Data
```bash
python generate_test_data.py
```
Creates: `realistic_exoplanet_test_data.csv`, `simple_exoplanet_test_data.csv`, `large_exoplanet_test_data.csv`, `edge_case_exoplanet_test_data.csv` â€” upload via â€œMake Predictions â†’ Upload CSVâ€.

## ğŸ“œ License & Acknowledgments
This project leverages NASAâ€™s publicly available exoplanet datasets and builds on recent research in exoplanet detection with ensemble learning.
- TransitDot/NOTES/ORACLE analytics
- JWST readiness scoring

References: `https://www.asc-csa.gc.ca/eng/satellites/neossat/`, `https://donnees-data.asc-csa.gc.ca/en/dataset/9ae3e718-8b6d-40b7-8aa4-858f00e84b30`, `https://www.asc-csa.gc.ca/eng/satellites/jwst/about.asp`.

## ğŸ›  Troubleshooting
- Models not loaded â†’ refresh page; ensure joblib files or run training
- SimpleImputer not fitted â†’ open â€œMake Predictionsâ€ once or retrain
- External validation empty â†’ check filenames/labels as noted above

## ğŸ“„ License
MIT
